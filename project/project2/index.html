<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Joel James" />
    
    <link rel="shortcut icon" type="image/x-icon" href="../../img/favicon.ico">
    <title>Project 2</title>
    <meta name="generator" content="Hugo 0.79.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">
      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../../post/">BLOG</a></li>
        
        <li><a href="../../projects/">PROJECTS</a></li>
        
        <li><a href="../../resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../../project/project2/">Project 2</a></strong>
          </h3>
        </div>
 
<div class="blog-title">
          <h4>
         December 11, 2020 
            &nbsp;&nbsp;
            
          </h4>
        </div>

        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<pre class="r"><code>#Class diags 

class_diag&lt;-function(probs,truth){
  
  tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord&lt;-order(probs, decreasing=TRUE)
  probs &lt;- probs[ord]; truth &lt;- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
  TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
  
  n &lt;- length(TPR)
  auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}</code></pre>
<pre class="r"><code>library(ggplot2)
library(ggrepel)
library(tidyr)
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ tibble  3.0.3     ✓ dplyr   1.0.1
## ✓ readr   1.3.1     ✓ stringr 1.4.0
## ✓ purrr   0.3.4     ✓ forcats 0.5.0</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(dplyr)
library(vegan)</code></pre>
<pre><code>## Loading required package: permute</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## This is vegan 2.5-6</code></pre>
<pre class="r"><code>library(plotROC)
library(sandwich)
library(lmtest)</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre class="r"><code>Wage_Gap &lt;- read.csv(&quot;Wage.csv&quot;, header = TRUE)
ncol(Wage_Gap)</code></pre>
<pre><code>## [1] 7</code></pre>
<pre class="r"><code>nrow(Wage_Gap)</code></pre>
<pre><code>## [1] 534</code></pre>
<p><em>Introduction: The dataset I have chosen to work with comes from the U.S. Census Bureau which includes information on wage earnings from 1989. This dataset has 7 variables with 534 observations. The main variable I will be looking at throughout this project is wage in dollars/hour to see the effect of different determinants on hourly wage. The other variables I will be looking at include the paticipants' age, years of education, and their predicted work experience which is caculated by the following equation: work experience= age-(years of education)-6. I will also be looking at the participants' gender, union status, and marriage status to see if these determinants also have a significant effect on wage earnings.</em></p>
<pre class="r"><code>#MANOVA
man1&lt;-manova(cbind(wage,education,experience,age)~gender, data=Wage_Gap)
summary(man1)</code></pre>
<pre><code>##            Df   Pillai approx F num Df den Df    Pr(&gt;F)    
## gender      1 0.072867   10.394      4    529 4.131e-08 ***
## Residuals 532                                              
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#ANOVA
summary.aov(man1)</code></pre>
<pre><code>##  Response wage :
##              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## gender        1   593.7  593.71  23.426 1.703e-06 ***
## Residuals   532 13483.0   25.34                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response education :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## gender        1    0.0   0.015  0.0022 0.9627
## Residuals   532 3645.8   6.853               
## 
##  Response experience :
##              Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## gender        1    462  462.31   3.028 0.08242 .
## Residuals   532  81224  152.68                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response age :
##              Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## gender        1    459  459.50  3.3563 0.06751 .
## Residuals   532  72835  136.91                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Post-Hoc
pairwise.t.test(Wage_Gap$wage,Wage_Gap$gender, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Wage_Gap$wage and Wage_Gap$gender 
## 
##      female 
## male 1.7e-06
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>#Type-1-Error
1-(1-.05)^5</code></pre>
<pre><code>## [1] 0.2262191</code></pre>
<pre class="r"><code>#Bonferonni correction
0.05/5</code></pre>
<pre><code>## [1] 0.01</code></pre>
<pre class="r"><code>#Assumptions
WWage_Gap &lt;- Wage_Gap%&gt;%select(gender,wage,education,experience,age)
ggplot(WWage_Gap, aes(x = wage, y = education)) +
 geom_point(alpha = .5) + geom_density_2d(h=2) + coord_fixed() + facet_wrap(~gender)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>covmats&lt;-WWage_Gap%&gt;%group_by(gender)%&gt;%do(covs=cov(.[2:5]))
for(i in 1:2){print(covmats$covs[i])}</code></pre>
<pre><code>## [[1]]
##                  wage  education  experience        age
## wage       22.2794700   5.049372   0.1829637   5.197563
## education   5.0493724   5.901037 -10.7868685  -4.934610
## experience  0.1829637 -10.786869 159.0825360 148.604399
## age         5.1975632  -4.934610 148.6043995 143.995049
## 
## [[1]]
##                 wage  education experience        age
## wage       27.940258   5.232571   11.91468  17.147255
## education   5.232571   7.659530  -12.00299  -4.343462
## experience 11.914684 -12.002992  147.24880 135.245807
## age        17.147255  -4.343462  135.24581 130.902345</code></pre>
<p><em>The first step of my analysis was to conduct a MANOVA test to determine the effect of gender on wage, experience, education, and age. Significant differences were found between the two genders (Pillai Trace= .07, psuedo F= 4, 529, p&lt;.01). Univariate ANOVAs then were conducted on each of the dependent variables, after which I found that only wage had a significant effect on gender (F(1, 532) = 23.426, p&lt;,01). A post hoc t-test was also done out of convention but this was not necessary since there are only two genders. After excluding the t-test, there were a total of five tests done including the first MANOVA test and the four ANOVA tests. This means that we need to use a Bonferroni significance level of .01 instead of .05 which was already applied to the previous tests. There is also a 22.6% chance of a Type 1 Error. None of the tests' outcomes changed after applying the Bonferroni correction. After eyballing some of the assumptions, it does not seem like many of them will be met. We might be able to assume that we have random samples and independent observations. We can also say thateach group has n=25+ and that there may be linear relationships between dependent variables. But the multivariate normality and convariance suggest that most of our assumptions have not been met which shows how restrictive MANOVAs are.</em></p>
<pre class="r"><code>#Ho= Mean wages are the same between men and women
#Ha= Mean wages are not the same between men and women
summary(aov(wage~gender,data=Wage_Gap))</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## gender        1    594   593.7   23.43 1.7e-06 ***
## Residuals   532  13483    25.3                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Ho= Group mean wages are the same across the two genders
#Ha= Group mean wages are not the same across the two genders
obs_F&lt;-23.43 
Fs&lt;-replicate(1000,{
  new&lt;-Wage_Gap%&gt;%mutate(wage=sample(wage)) 
  SSW&lt;- new%&gt;%group_by(gender)%&gt;%summarize(SSW=sum((wage-mean(wage))^2))%&gt;%
    summarize(sum(SSW))%&gt;%pull
  SSB&lt;- new%&gt;%mutate(mean=mean(wage))%&gt;%group_by(gender)%&gt;%mutate(groupmean=mean(wage))%&gt;%
    summarize(SSB=sum((mean-groupmean)^2))%&gt;%summarize(sum(SSB))%&gt;%pull
  (SSB/1)/(SSW/532)
})
hist(Fs, prob=T); abline(v = obs_F, col=&quot;red&quot;,add=T)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>#P-value
mean(Fs&gt;obs_F)</code></pre>
<pre><code>## [1] 0</code></pre>
<p><em>The next step in my analysis was to conduct a ANOVA/F-statistic randomization test. This was done by first conducting an ANOVA test between wage and gender. I discovered that there is a significant difference in wage between men and women (F= 23.43, df= 1, 532, p&lt;.05). I then scrambled the data to destroy the association between gender and wage and found a null F-statistic where wage is the same between all genders. I then compared my actual F-statistic of 23.43 to this null F-statistic as seen in the histogram and found that my actual F-statistic is so far from the null F-statistic that it cannot even be seen. After comparing the two F-statistics, I also found a p-value=0. From this data, I can reject the null hypothesis and conclude that wages differ between the genders.</em></p>
<pre class="r"><code>Wage_Gap1 &lt;- Wage_Gap%&gt;%mutate(Married=ifelse(married==&quot;yes&quot;,1,0))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(Gender=ifelse(gender==&quot;male&quot;,1,0))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(Union=ifelse(union==&quot;yes&quot;,1,0))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(education_c = education - mean(education, na.rm = T))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(experience_c = experience - mean(experience, na.rm = T))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(age_c = age - mean(age, na.rm = T))
Wage_Gap1 &lt;- Wage_Gap1%&gt;%mutate(wage_c = wage - mean(wage, na.rm = T))

#Linear regression model
linear &lt;- lm(wage_c~gender*experience_c, data = Wage_Gap1)
summary(linear)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage_c ~ gender * experience_c, data = Wage_Gap1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.564 -3.476 -1.194  2.361 36.642 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             -1.14637    0.31993  -3.583 0.000371 ***
## gendermale               2.18654    0.43474   5.030 6.74e-07 ***
## experience_c             0.00115    0.02534   0.045 0.963809    
## gendermale:experience_c  0.07977    0.03506   2.275 0.023310 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.992 on 530 degrees of freedom
## Multiple R-squared:  0.06191,    Adjusted R-squared:  0.0566 
## F-statistic: 11.66 on 3 and 530 DF,  p-value: 2.079e-07</code></pre>
<pre class="r"><code>#Plot of the regression model
ggplot(Wage_Gap1, aes(x = experience_c, y = wage_c, group = gender)) + 
  geom_point(aes(color=gender)) + geom_smooth(method=&quot;lm&quot;, se=F,fullrange=T,aes(color=gender))</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>#Equal variance assumption
resids&lt;-linear$residuals
fitvals&lt;-linear$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color=&#39;red&#39;)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre class="r"><code>#Normally distributed residulas assumption
ggplot()+geom_histogram(aes(resids), bins=20)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-5-3.png" width="672" /></p>
<pre class="r"><code>#Linearity assumption
ggplot(Wage_Gap1,aes(experience_c,wage_c,color=gender))+geom_point()</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-5-4.png" width="672" /></p>
<pre class="r"><code>#Robust Standard Errors
coeftest(linear, vcov=vcovHC(linear))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                           Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)             -1.1463688  0.3126248 -3.6669 0.0002703 ***
## gendermale               2.1865387  0.4413553  4.9541 9.786e-07 ***
## experience_c             0.0011501  0.0276886  0.0415 0.9668829    
## gendermale:experience_c  0.0797652  0.0380319  2.0973 0.0364379 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><em>In the next part of my analysis, I performed a linear regression model using gender and experience to predict wage. The predicted wage of a female with average job experience is -1.14637 dollars/hour. Females get an increase of 0.00115 dollars/hour for every increase in one year of job experience on average. For people of average experience, males make 2.19 more dollars/hour than women. Finally, the slope for experience on wage is 0.07977 higher for men than for women. I can also conclude that 5.66% of the variation in my outcome is explained by this model. I then made a plot of the linear regression model and checked for assumptions. Based on the graphs, it seems that I was able to meet the linearity and equal variance assumption but my residuals were not normally distributed. Nevertheless, I conducted a regression with robust standard errors which is used for data that has heteroskedasticity. With robust standard erros, my coefficient estimates were relatively the same compared to the linear regression model and no tests changed in significance. This makes sense given that my data exhibited homoskedasticity to start with.</em></p>
<pre class="r"><code>#Bootstrap
set.seed(1234)
fit1&lt;-lm(wage_c~gender*experience_c, data = Wage_Gap1)
resids&lt;-fit1$residuals 
fitted&lt;-fit1$fitted.values 
resid_resamp&lt;-replicate(5000,{
  new_resids&lt;-sample(resids,replace=TRUE) 
  Wage_Gap1$new_y&lt;-fitted+new_resids
  fit1&lt;-lm(new_y~gender*experience_c, data = Wage_Gap1) 
  coef(fit1) 
})
resid_resamp%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>##   (Intercept) gendermale experience_c gendermale:experience_c
## 1   0.3166713  0.4323944   0.02496104              0.03505083</code></pre>
<p><em>In the next step of my analysis, I ran the linear regression model again but with bootstrapped standard errors which is used when the data is heteroskedastic and non-normal. I bootsrapped by resamplng residuals. In general, increasing standard error causes the test-statistic to decrease, like the t-value for example, which means that our p-value increases. Therefore, as standard error increasess, we are less likely to reject our null hypothesis. When comparing the SEs of the bootstrapped model to the original model, the bootstrapped model has lower SE values, and thus lower p-values, for the intercept, gendermale, experience_c, and gendermale:experience_c. When comparing the SEs of the bootstrapped model to the robust model, the bootstrapped model has a higher SE value, and thus higher p-value, for the intercept only. The bootstrapped model also has lower SE values, and thus lower p-values, for gendermale, experience_c, and gendermale:experience_c compared to the original model.</em></p>
<pre class="r"><code>library(plotROC)
Wage_Gap2&lt;-Wage_Gap1%&gt;%mutate(Wage=ifelse(wage&gt;7.25,1,0))
Wage_Gap2&lt;-Wage_Gap2%&gt;%mutate(Wages=ifelse(wage&gt;7.25,&quot;high&quot;,&quot;low&quot;))
#Logistic Regression Model
glmfit&lt;-glm(Wage~experience+education, data=Wage_Gap2, family=&quot;binomial&quot;)
coeftest(glmfit)</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##               Estimate Std. Error z value  Pr(&gt;|z|)    
## (Intercept) -5.0326277  0.6560562 -7.6710 1.706e-14 ***
## experience   0.0459700  0.0085503  5.3764 7.598e-08 ***
## education    0.3413627  0.0444370  7.6819 1.567e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Coefficients
exp(coef(glmfit))%&gt;%data.frame()</code></pre>
<pre><code>##                       .
## (Intercept) 0.006521651
## experience  1.047042963
## education   1.406863402</code></pre>
<pre class="r"><code>#Confussion Matrix
prob&lt;-predict(glmfit,type=&quot;response&quot;)
pred&lt;-ifelse(prob&gt;.5,1,0)
table(truth=Wage_Gap2$Wage, prediction=pred) %&gt;% addmargins</code></pre>
<pre><code>##      prediction
## truth   0   1 Sum
##   0   153  89 242
##   1    89 203 292
##   Sum 242 292 534</code></pre>
<pre class="r"><code>#Accuracy
(153+203)/534</code></pre>
<pre><code>## [1] 0.6666667</code></pre>
<pre class="r"><code>#Sensitivity
(203)/292</code></pre>
<pre><code>## [1] 0.6952055</code></pre>
<pre class="r"><code>#Specificity
(153)/242</code></pre>
<pre><code>## [1] 0.6322314</code></pre>
<pre class="r"><code>#Precision
(203)/292</code></pre>
<pre><code>## [1] 0.6952055</code></pre>
<pre class="r"><code>#AUC with package
class_diag(prob, Wage_Gap2$Wage)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.6666667 0.6952055 0.6322314 0.6952055 0.7203173</code></pre>
<pre class="r"><code>#Density Plot
ggplot(Wage_Gap2, aes(predict(glmfit), fill=Wages)) +
  geom_density(alpha=0.3) + geom_vline(xintercept=0, lty=2)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>#ROC Plot
ROCplot&lt;-ggplot(Wage_Gap2)+geom_roc(aes(d=Wage,m=prob), n.cuts=0)+
  geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2)
ROCplot</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code>#AUC
calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.7203173</code></pre>
<p><em>During the next part of my analysis, I ran a logistic regression model to see the effect of experience and education on wage. For the purposes of this study, I decided that wages lower than 7.25 dollars/hour were low and wages above this amount were high. The predicted odds of having a high hourly wage is 0.007 with no education and no experience. While controlling for education, every one additional year in work experience multiplies the odds of having a high wage by a factor of 1.047. While controlling for expereince, every one additional year in education multiplies the odds of having a high wage by a factor of 1.407. After running the regression, I then made a confusion matrix too see how my model's predictions compare to the true outcomes. My model correctly identified cases (accuracy) 66.67% of the time. The proportion of high wages correctly classified (sensitivity) is 69.52%. The proportion of low wages correctly classified (specificity) is 63.22%. The proportion classified as high low wages who actually are (precision) is 69.52%. The AUC, which was calculated using class_auc and class_diag, is 0.720 which means the model is a fair predictor of high and low wages. I then created an density plot of the log-odds and an ROC curve of the data. The ROC curve is fairly descent considering that it isn’t a straight line which indicates the model can fairly distinguish between cases. This conclusion is backed up by the AUC as previously stated.</em></p>
<pre class="r"><code>library(tidyverse); library(lmtest)
library(plotROC)
library(pROC)</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:plotROC&#39;:
## 
##     ggroc</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>library(glmnet)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 4.0-2</code></pre>
<pre class="r"><code>#Logistic regression with all variables
sixfit&lt;-glm(Wage~age+Gender+experience+education+Married+Union,data=Wage_Gap2,family=&quot;binomial&quot;)
sixprob&lt;-predict(sixfit,data=&quot;response&quot;)
class_diag(sixprob,Wage_Gap2$Wage)</code></pre>
<pre><code>##         acc      sens      spec       ppv      auc
## 1 0.6666667 0.5513699 0.8057851 0.7740385 0.767654</code></pre>
<pre class="r"><code>#10-fold CV
set.seed(1234)
k=10
data&lt;-Wage_Gap2[sample(nrow(Wage_Gap2)),] 
folds&lt;-cut(seq(1:nrow(data)),breaks=k,labels=F)
diags&lt;-NULL
for(i in 1:k){
  train&lt;-data[folds!=i,]
  test&lt;-data[folds==i,]
  truth&lt;-test$Wage
  fit&lt;-glm(Wage~age+Gender+experience+education+Married+Union,data=Wage_Gap2,family=&quot;binomial&quot;)
  probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)
  diags&lt;-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.6889588 0.7240142 0.6442311 0.7126771 0.7627933</code></pre>
<pre class="r"><code>#LASSO
y &lt;- as.matrix(Wage_Gap2$Wage)
x &lt;- model.matrix(Wage~Gender+age+experience+education+Married+
                    Union,data=Wage_Gap2,family=&quot;binomial&quot;)[, -1]
x &lt;- scale(x)
cv &lt;- cv.glmnet(x, y, family = &quot;binomial&quot;)
lasso_fit &lt;- glmnet(x, y, family = &quot;binomial&quot;, lambda = cv$lambda.1se)
coef(lasso_fit)</code></pre>
<pre><code>## 7 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     s0
## (Intercept) 0.20233518
## Gender      0.09987542
## age         0.21270324
## experience  .         
## education   0.43800001
## Married     .         
## Union       0.19948095</code></pre>
<pre class="r"><code>#10-fold with LASSO
set.seed(1234)
k=10
data&lt;-Wage_Gap2[sample(nrow(Wage_Gap2)),]
folds&lt;-cut(seq(1:nrow(data)),breaks=k,labels=F) 
diags&lt;-NULL
for(i in 1:k){
  train&lt;-data[folds!=i,]
  test&lt;-data[folds==i,]
  truth&lt;-test$Wage
  fit&lt;-glm(Wage~Gender+age+education+Union,data=Wage_Gap2,family=&quot;binomial&quot;)
  probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)
  diags&lt;-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.6890636 0.7246162 0.6418135 0.7104331 0.7582767</code></pre>
<p><em>In the last part of my analysis, I conducted another logistic regression with all the variables. My model correctly identified cases (accuracy) 66.67% of the time. The proportion of high wages correctly classified (sensitivity) is 55.14%. The proportion of low wages correctly classified (specificity) is 80.58%. The proportion classified as high low wages who actually are (precision) is 77.40%. The AUC is 0.768 which means the model is a fair predictor of high and low wages. In fact, this model is better than the previous model with an AUC of .720. I then performed a 10-fold CV on the same model. The new model correctly identified cases (accuracy) 69.90% of the time. The proportion of high wages correctly classified (sensitivity) is 72.40%. The proportion of low wages correctly classified (specificity) is 64.42%. The proportion classified as high low wages who actually are (precision) is 71.27%. The AUC is 0.763 which means this model is also a fair predictor of high and low wages. However, since this AUC is lower than the in-sample metrics, we have evidence of overfitting. To reduce overfitting, I performed a LASSO on this model which retained gender, age, education, and union status as significant variables. Another 10-fold CV with the LASSO variables was performed which calculated an AUC of 0.758. Since this AUC is lower than the other two values, we can surmise that LASSO does not help create a better model. However the LASSO model is better at predicting wage than a logistic regression using experience and education alone. </em></p>

            
        <hr>         <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div> 
            </div>
          </div>

   <hr>  <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 
        </div>
      </div>
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../../js/docs.min.js"></script>
<script src="../../js/main.js"></script>

<script src="../../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
